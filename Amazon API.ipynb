{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import json\n",
    "import time\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReviews(ISBN,return_dict):\n",
    "    url = \"https://amazon24.p.rapidapi.com/api/review/\"+ISBN\n",
    "\n",
    "    querystring = {\"page\":\"1\",\"country\":\"US\",\"sortBy\":\"helpful\"}\n",
    "\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"ab5b986be2mshdf5fb3e8e87e3e0p1c84abjsn45ac59a8e4c0\",\n",
    "        \"X-RapidAPI-Host\": \"amazon24.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    data = response.json()\n",
    "\n",
    "    reviews = data['docs'][0:5]\n",
    "    review_list = []\n",
    "    for review in reviews:\n",
    "        \n",
    "        review_list.append(review['text'])\n",
    "        \n",
    "    \n",
    "    if len(review_list) == 0:\n",
    "        review_list = ''\n",
    "        \n",
    "    return_dict['reviews'] = review_list\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetails(ISBN,return_dict):\n",
    "\n",
    "    url = \"https://amazon24.p.rapidapi.com/api/product/\"+ISBN\n",
    "\n",
    "    querystring = {\"country\":\"US\"}\n",
    "\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"161c227e7bmshe1117c8b5dd8331p1b2a19jsn5768fbc0a29d\",\n",
    "        \"X-RapidAPI-Host\": \"amazon24.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "    data = response.json()\n",
    "    \n",
    "    genre_list = []\n",
    "\n",
    "    try: \n",
    "        text = data['product_details']['Best Sellers Rank']  #genre\n",
    "\n",
    "        text = text.split('#')\n",
    "        text = [x for x in text if x!='']\n",
    "        for t in text:\n",
    "            li = t.split('in',1)\n",
    "            rank = li[0]\n",
    "            genre = li[1]    \n",
    "            parenthesis = re.search(\"[\\(.*\\)]\",genre)\n",
    "            if parenthesis:\n",
    "                start = parenthesis.start()\n",
    "                genre = genre[:start]\n",
    "\n",
    "            if genre.lower().strip() != 'books':\n",
    "                genre_list.append(genre.strip())\n",
    "\n",
    "    except:\n",
    "        categories = data['breadcrumbs']\n",
    "        for cat in categories:\n",
    "            cat = cat['name']\n",
    "            if cat.lower().strip() != 'books':\n",
    "                genre_list.append(cat.strip())\n",
    "    \n",
    "    if len(genre_list) == 0:\n",
    "        genre_list = ''\n",
    "        \n",
    "    return_dict['genre'] = genre_list\n",
    "    \n",
    "    return return_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirubha\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3457: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0425166619</td>\n",
       "      <td>Toxin</td>\n",
       "      <td>Robin Cook</td>\n",
       "      <td>1999</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0425166619.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0425166619.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0425166619.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>061312975X</td>\n",
       "      <td>This Present Darkness</td>\n",
       "      <td>Frank E. Peretti</td>\n",
       "      <td>1999</td>\n",
       "      <td>Sagebrush Bound</td>\n",
       "      <td>http://images.amazon.com/images/P/061312975X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/061312975X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/061312975X.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0765341972</td>\n",
       "      <td>The Mothman Prophecies</td>\n",
       "      <td>John A. Keel</td>\n",
       "      <td>2002</td>\n",
       "      <td>Tor Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0765341972.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0765341972.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0765341972.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0590514776</td>\n",
       "      <td>Meet the Stars of Buffy the Vampire Slayer</td>\n",
       "      <td>Stefanie Scott</td>\n",
       "      <td>1998</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>http://images.amazon.com/images/P/0590514776.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0590514776.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0590514776.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0373037430</td>\n",
       "      <td>Rush to the Altar  (Twin Brides)</td>\n",
       "      <td>Rebecca Winters</td>\n",
       "      <td>2003</td>\n",
       "      <td>Harlequin</td>\n",
       "      <td>http://images.amazon.com/images/P/0373037430.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0373037430.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0373037430.0...</td>\n",
       "      <td>https://www.goodreads.com/search?utf8=%E2%9C%9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ISBN                                         Book-Title  \\\n",
       "0    0195153448                                Classical Mythology   \n",
       "1    0002005018                                       Clara Callan   \n",
       "2    0060973129                               Decision in Normandy   \n",
       "3    0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4    0393045218                             The Mummies of Urumchi   \n",
       "..          ...                                                ...   \n",
       "995  0425166619                                              Toxin   \n",
       "996  061312975X                              This Present Darkness   \n",
       "997  0765341972                             The Mothman Prophecies   \n",
       "998  0590514776         Meet the Stars of Buffy the Vampire Slayer   \n",
       "999  0373037430                   Rush to the Altar  (Twin Brides)   \n",
       "\n",
       "              Book-Author Year-Of-Publication                   Publisher  \\\n",
       "0      Mark P. O. Morford                2002     Oxford University Press   \n",
       "1    Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       "2            Carlo D'Este                1991             HarperPerennial   \n",
       "3        Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       "4         E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       "..                    ...                 ...                         ...   \n",
       "995            Robin Cook                1999    Berkley Publishing Group   \n",
       "996      Frank E. Peretti                1999             Sagebrush Bound   \n",
       "997          John A. Keel                2002                   Tor Books   \n",
       "998        Stefanie Scott                1998                  Scholastic   \n",
       "999       Rebecca Winters                2003                   Harlequin   \n",
       "\n",
       "                                           Image-URL-S  \\\n",
       "0    http://images.amazon.com/images/P/0195153448.0...   \n",
       "1    http://images.amazon.com/images/P/0002005018.0...   \n",
       "2    http://images.amazon.com/images/P/0060973129.0...   \n",
       "3    http://images.amazon.com/images/P/0374157065.0...   \n",
       "4    http://images.amazon.com/images/P/0393045218.0...   \n",
       "..                                                 ...   \n",
       "995  http://images.amazon.com/images/P/0425166619.0...   \n",
       "996  http://images.amazon.com/images/P/061312975X.0...   \n",
       "997  http://images.amazon.com/images/P/0765341972.0...   \n",
       "998  http://images.amazon.com/images/P/0590514776.0...   \n",
       "999  http://images.amazon.com/images/P/0373037430.0...   \n",
       "\n",
       "                                           Image-URL-M  \\\n",
       "0    http://images.amazon.com/images/P/0195153448.0...   \n",
       "1    http://images.amazon.com/images/P/0002005018.0...   \n",
       "2    http://images.amazon.com/images/P/0060973129.0...   \n",
       "3    http://images.amazon.com/images/P/0374157065.0...   \n",
       "4    http://images.amazon.com/images/P/0393045218.0...   \n",
       "..                                                 ...   \n",
       "995  http://images.amazon.com/images/P/0425166619.0...   \n",
       "996  http://images.amazon.com/images/P/061312975X.0...   \n",
       "997  http://images.amazon.com/images/P/0765341972.0...   \n",
       "998  http://images.amazon.com/images/P/0590514776.0...   \n",
       "999  http://images.amazon.com/images/P/0373037430.0...   \n",
       "\n",
       "                                           Image-URL-L  \\\n",
       "0    http://images.amazon.com/images/P/0195153448.0...   \n",
       "1    http://images.amazon.com/images/P/0002005018.0...   \n",
       "2    http://images.amazon.com/images/P/0060973129.0...   \n",
       "3    http://images.amazon.com/images/P/0374157065.0...   \n",
       "4    http://images.amazon.com/images/P/0393045218.0...   \n",
       "..                                                 ...   \n",
       "995  http://images.amazon.com/images/P/0425166619.0...   \n",
       "996  http://images.amazon.com/images/P/061312975X.0...   \n",
       "997  http://images.amazon.com/images/P/0765341972.0...   \n",
       "998  http://images.amazon.com/images/P/0590514776.0...   \n",
       "999  http://images.amazon.com/images/P/0373037430.0...   \n",
       "\n",
       "                                                   url  \n",
       "0    https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "1    https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "2    https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "3    https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "4    https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "..                                                 ...  \n",
       "995  https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "996  https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "997  https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "998  https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "999  https://www.goodreads.com/search?utf8=%E2%9C%9...  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('.\\\\Project\\\\CZ4125---Data-Products---BookWorms\\\\data\\\\Books.csv')\n",
    "df[:1000] #just scraping for 1000 ISBNs (due to API free version usage limitations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = df[800:1000] #keep increasing first limit by 200 \n",
    "#--> have to run each subset separately due to API per minute limit restrictions \n",
    "subset_df= subset_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row:  0 - done!\n",
      "row:  1 - done!\n",
      "row:  2 - done!\n",
      "row:  3 - done!\n",
      "row:  4 - done!\n",
      "row:  5 - done!\n",
      "row:  6 - done!\n",
      "row:  7 - done!\n",
      "row:  8 - done!\n",
      "row:  9 - done!\n",
      "row:  10 - done!\n",
      "row:  11 - done!\n",
      "row:  12 - done!\n",
      "row:  13 - done!\n",
      "row:  14 - done!\n",
      "row:  15 - done!\n",
      "row:  16 - done!\n",
      "row:  17 - done!\n",
      "row:  18 - done!\n",
      "row:  19 - done!\n",
      "row:  20 - done!\n",
      "row:  21 - done!\n",
      "row:  22 - done!\n",
      "row:  23 - done!\n",
      "row:  24 - done!\n",
      "row:  25 - done!\n",
      "row:  26 - done!\n",
      "row:  27 - done!\n",
      "row:  28 - done!\n",
      "row:  29 - done!\n",
      "row:  30 - done!\n",
      "row:  31 - done!\n",
      "row:  32 - done!\n",
      "row:  33 - done!\n",
      "row:  34 - done!\n",
      "row:  35 - done!\n",
      "row:  36 - done!\n",
      "row:  37 - done!\n",
      "row:  38 - done!\n",
      "row:  39 - done!\n",
      "row:  40 - done!\n",
      "row:  41 - done!\n",
      "row:  42 - done!\n",
      "row:  43 - done!\n",
      "row:  44 - done!\n",
      "row:  45 - done!\n",
      "row:  46 - done!\n",
      "row:  47 - done!\n",
      "row:  48 - done!\n",
      "row:  49 - done!\n",
      "row:  50 - done!\n",
      "row:  51 - done!\n",
      "row:  52 - done!\n",
      "row:  53 - done!\n",
      "row:  54 - done!\n",
      "row:  55 - done!\n",
      "row:  56 - done!\n",
      "row:  57 - done!\n",
      "row:  58 - done!\n",
      "row:  59 - done!\n",
      "row:  60 - done!\n",
      "row:  61 - done!\n",
      "row:  62 - done!\n",
      "row:  63 - done!\n",
      "row:  64 - done!\n",
      "row:  65 - done!\n",
      "row:  66 - done!\n",
      "row:  67 - done!\n",
      "row:  68 - done!\n",
      "row:  69 - done!\n",
      "row:  70 - done!\n",
      "row:  71 - done!\n",
      "row:  72 - done!\n",
      "row:  73 - done!\n",
      "row:  74 - done!\n",
      "row:  75 - done!\n",
      "row:  76 - done!\n",
      "row:  77 - done!\n",
      "row:  78 - done!\n",
      "row:  79 - done!\n",
      "row:  80 - done!\n",
      "row:  81 - done!\n",
      "row:  82 - done!\n",
      "row:  83 - done!\n",
      "row:  84 - done!\n",
      "row:  85 - done!\n",
      "row:  86 - done!\n",
      "row:  87 - done!\n",
      "row:  88 - done!\n",
      "row:  89 - done!\n",
      "row:  90 - done!\n",
      "row:  91 - done!\n",
      "row:  92 - done!\n",
      "row:  93 - done!\n",
      "row:  94 - done!\n",
      "row:  95 - done!\n",
      "row:  96 - done!\n",
      "row:  97 - done!\n",
      "row:  98 - done!\n",
      "row:  99 - done!\n",
      "row:  100 - done!\n",
      "row:  101 - done!\n",
      "row:  102 - done!\n",
      "row:  103 - done!\n",
      "row:  104 - done!\n",
      "row:  105 - done!\n",
      "row:  106 - done!\n",
      "row:  107 - done!\n",
      "row:  108 - done!\n",
      "row:  109 - done!\n",
      "row:  110 - done!\n",
      "row:  111 - done!\n",
      "row:  112 - done!\n",
      "row:  113 - done!\n",
      "row:  114 - done!\n",
      "row:  115 - done!\n",
      "row:  116 - done!\n",
      "row:  117 - done!\n",
      "row:  118 - done!\n",
      "row:  119 - done!\n",
      "row:  120 - done!\n",
      "row:  121 - done!\n",
      "row:  122 - done!\n",
      "row:  123 - done!\n",
      "row:  124 - done!\n",
      "row:  125 - done!\n",
      "row:  126 - done!\n",
      "row:  127 - done!\n",
      "row:  128 - done!\n",
      "row:  129 - done!\n",
      "row:  130 - done!\n",
      "row:  131 - done!\n",
      "row:  132 - done!\n",
      "row:  133 - done!\n",
      "row:  134 - done!\n",
      "row:  135 - done!\n",
      "row:  136 - done!\n",
      "row:  137 - done!\n",
      "row:  138 - done!\n",
      "row:  139 - done!\n",
      "row:  140 - done!\n",
      "row:  141 - done!\n",
      "row:  142 - done!\n",
      "row:  143 - done!\n",
      "row:  144 - done!\n",
      "row:  145 - done!\n",
      "row:  146 - done!\n",
      "row:  147 - done!\n",
      "row:  148 - done!\n",
      "row:  149 - done!\n",
      "row:  150 - done!\n",
      "row:  151 - done!\n",
      "row:  152 - done!\n",
      "row:  153 - done!\n",
      "row:  154 - done!\n",
      "row:  155 - done!\n",
      "row:  156 - done!\n",
      "row:  157 - done!\n",
      "row:  158 - done!\n",
      "row:  159 - done!\n",
      "row:  160 - done!\n",
      "row:  161 - done!\n",
      "row:  162 - done!\n",
      "row:  163 - done!\n",
      "row:  164 - done!\n",
      "row:  165 - done!\n",
      "row:  166 - done!\n",
      "row:  167 - done!\n",
      "row:  168 - done!\n",
      "row:  169 - done!\n",
      "row:  170 - done!\n",
      "row:  171 - done!\n",
      "row:  172 - done!\n",
      "row:  173 - done!\n",
      "row:  174 - done!\n",
      "row:  175 - done!\n",
      "row:  176 - done!\n",
      "row:  177 - done!\n",
      "row:  178 - done!\n",
      "row:  179 - done!\n",
      "row:  180 - done!\n",
      "row:  181 - done!\n",
      "row:  182 - done!\n",
      "row:  183 - done!\n",
      "row:  184 - done!\n",
      "row:  185 - done!\n",
      "row:  186 - done!\n",
      "row:  187 - done!\n",
      "row:  188 - done!\n",
      "row:  189 - done!\n",
      "row:  190 - done!\n",
      "row:  191 - done!\n",
      "row:  192 - done!\n",
      "row:  193 - done!\n",
      "row:  194 - done!\n",
      "row:  195 - done!\n",
      "row:  196 - done!\n",
      "row:  197 - done!\n",
      "row:  198 - done!\n",
      "row:  199 - done!\n",
      "need to run from row:  200\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "rejected_rows = []\n",
    "json_obj = []\n",
    "\n",
    "\n",
    "for row in range(len(subset_df)):\n",
    "    \n",
    "    ISBN = subset_df.loc[row,'ISBN']\n",
    "    Book_Title = subset_df.loc[row,'Book-Title']\n",
    "    Book_Author = subset_df.loc[row,'Book-Author']\n",
    "    img_URL = subset_df.loc[row,'Image-URL-L']\n",
    "\n",
    "    return_dict = {}\n",
    "    return_dict['ISBN'] = ISBN\n",
    "    return_dict['Book-Title'] = Book_Title\n",
    "    return_dict['Book-Author'] = Book_Author\n",
    "    return_dict['summary'] = ''\n",
    "    return_dict['Image-URL-L'] = img_URL\n",
    "    return_dict['reviews'] = return_dict['genre'] = ''\n",
    "\n",
    "    return_dict = getReviews(ISBN,return_dict)\n",
    "    return_dict = getDetails(ISBN,return_dict)\n",
    "\n",
    "    #write to json file \n",
    "    json_obj.append(return_dict)\n",
    "    with open('data.json','w+')as outfile:\n",
    "        json.dump(json_obj,outfile)\n",
    "\n",
    "    print('row: ',str(row),'- done!')\n",
    "\n",
    "    count+=1\n",
    "    \n",
    "    if count%10==0:\n",
    "        time.sleep(60)\n",
    "    \n",
    "    if count == 200:\n",
    "        print('need to run from row: ',row+1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected_rows #rows unable to be scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding json file to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = 'mongodb+srv://tartiniglia:W.I.T.C.H.@atlascluster.tv8xjir.mongodb.net/?retryWrites=true&w=majority'\n",
    "client = pymongo.MongoClient(conn_str, serverSelectionTimeoutMS=5000)\n",
    "mydb = client['bookEater']\n",
    "mycol = mydb['Books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1d18128ec48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data.json') as file:\n",
    "    file_data = json.load(file)\n",
    "mycol.insert_many(file_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
